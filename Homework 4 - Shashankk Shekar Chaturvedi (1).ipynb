{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac19b859",
   "metadata": {},
   "source": [
    "# Homework 4 – Programming:\n",
    "\n",
    "In this programming problem, you will get familiar with building a neural network using backpropagation. You will write a program that learns how to recognize the handwritten digits using stochastic gradient descent and the MNIST training data.\n",
    "The MNIST database (Modified National Institute of Standards and Technology database is a large database of handwritten digits that is commonly used for training various image processing systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8517",
   "metadata": {},
   "source": [
    "### Step 1(a): Data Acquisition\n",
    "\n",
    "In this step, we will download the MNIST dataset and extract the files. The MNIST dataset is a large database of handwritten digits that is commonly used for training various image processing systems. The dataset is split into 60,000 training images and 10,000 testing images.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcc90ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images:  (60000, 28, 28)\n",
      "Shape of training labels:  (60000, 10)\n",
      "Shape of testing images:  (10000, 28, 28)\n",
      "Shape of testing labels:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "# Function to read the MNIST dataset\n",
    "def load_data(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "# File paths\n",
    "train_images_file = 'MNIST/train-images-idx3-ubyte.gz'\n",
    "train_labels_file = 'MNIST/train-labels-idx1-ubyte.gz'\n",
    "test_images_file = 'MNIST/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_file = 'MNIST/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "# Load the data\n",
    "train_images = load_data(train_images_file)\n",
    "train_labels = load_data(train_labels_file)\n",
    "test_images = load_data(test_images_file)\n",
    "test_labels = load_data(test_labels_file)\n",
    "\n",
    "# Normalize the image data to [0, 1] range\n",
    "train_x = train_images.astype(np.float32) / 255\n",
    "test_x = test_images.astype(np.float32) / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_y = np.eye(10)[train_labels]\n",
    "test_y = np.eye(10)[test_labels]\n",
    "\n",
    "# Print the shapes\n",
    "print(\"Shape of training images: \", train_x.shape)\n",
    "print(\"Shape of training labels: \", train_y.shape)\n",
    "print(\"Shape of testing images: \", test_x.shape)\n",
    "print(\"Shape of testing labels: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a9da2",
   "metadata": {},
   "source": [
    "#### Discussion for Step 1(a)\n",
    "\n",
    "In Step 1(a), we downloaded the MNIST dataset and extracted the files. The MNIST dataset is a large database of handwritten digits that is commonly used for training various image processing systems. The dataset is split into 60,000 training images and 10,000 testing images.\n",
    "\n",
    "We used a Python function to load the data from the files. This function opens the file, reads the header information to determine the dimensions of the data, and then reads the data itself. The data is reshaped into the correct dimensions and returned as a numpy array.\n",
    "\n",
    "After loading the data, we printed the shapes of the training and testing datasets. This gives us an idea of the size of the datasets we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8405882",
   "metadata": {},
   "source": [
    "### Step 1(b): Data Visualization\n",
    "\n",
    "In this step, we will use the ‘matplotlib’ library to print out a random data with its label. This will help us understand what the dataset is.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72cb7374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display a random image from the training set\n",
    "random_index = np.random.randint(train_images.shape[0])\n",
    "plt.title(\"Label: {}\".format(train_labels[random_index]))\n",
    "plt.imshow(train_images[random_index], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e674140",
   "metadata": {},
   "source": [
    "#### Discussion for Step 1(b)\n",
    "\n",
    "In Step 1(b), we visualized the data to get a better understanding of what we are working with. We used the ‘matplotlib’ library to display a random image from the training set along with its label.\n",
    "\n",
    "Visualizing the data is an important step in any machine learning project. It allows us to see what the data looks like and can give us insights into the nature of the problem we are trying to solve. In this case, seeing an example of a handwritten digit and its corresponding label helps us understand the task of recognizing handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f0337",
   "metadata": {},
   "source": [
    "### Step 2(a): Normalize the pixel values of images\n",
    "\n",
    "In this step, we normalize the pixel values of the images to be between 0 and 1. Normalizing the pixel values helps in speeding up the training process and also prevents the model from getting stuck in local optima.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6b372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7b48a",
   "metadata": {},
   "source": [
    "This code simply divides all pixel values by 255 (since pixel values range from 0 to 255) to bring them into the range [0,1].\n",
    "\n",
    "### Discussion for Step 2(a):-\n",
    "\n",
    "Normalizing the pixel values of the images is an important preprocessing step in many machine learning algorithms. In the context of image data, pixel values are usually integers in the range [0, 255]. By dividing each pixel value by 255, we scale the data to the range [0, 1]. This can make the optimization process more efficient, as it can help the algorithm converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd7b54",
   "metadata": {},
   "source": [
    "### Step 2(b): One-Hot Encoding\n",
    "\n",
    "In this step, we convert the labels from categorical data into numerical values using one-hot encoding. One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. With one-hot, we convert each categorical value into a new categorical value and assign a binary value of 1 or 0. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55277d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform one-hot encoding\n",
    "def one_hot_encode(y):\n",
    "    n = np.max(y) + 1\n",
    "    return np.eye(n)[y]\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "train_labels = one_hot_encode(train_labels)\n",
    "test_labels = one_hot_encode(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cea3f6",
   "metadata": {},
   "source": [
    "### Discussion for Step 2(b):-\n",
    "\n",
    "One-hot encoding is a common technique used for dealing with categorical data in machine learning. In this step, I’ve performed one-hot encoding on the labels. One-hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. With one-hot encoding, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0. Each integer value is represented as a binary vector. This is crucial when we are dealing with categorical data like labels in classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc986",
   "metadata": {},
   "source": [
    "### Step 3(a): Implement a neural network model with one hidden layer\n",
    "\n",
    "In this step, we will implement a neural network model with one hidden layer. The hidden layer will have 64 neurons. We will use the Sigmoid function as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bfaf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid derivative function for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6c8ca",
   "metadata": {},
   "source": [
    "This code defines the sigmoid function and its derivative. The sigmoid function is used as the activation function in the hidden layer of the neural network. The derivative of the sigmoid function is used during the backpropagation step to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983dc911",
   "metadata": {},
   "source": [
    "### Discussion for Step 3(a)\n",
    "\n",
    "1. **Sigmoid Function**: The sigmoid function is an activation function that is used in neural networks to introduce non-linearity in the model. It maps any input value into a range between 0 and 1. This can be particularly useful when we want to predict probabilities, which naturally fall into this range. Mathematically, the sigmoid function is represented as:\n",
    "\n",
    "    $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "    Here, `x` is the input to the function. The sigmoid function outputs a value between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "2. **Sigmoid Derivative Function**: This function calculates the derivative of the sigmoid function. The derivative of the sigmoid function is used during the backpropagation step of training a neural network. It is used to calculate the gradients of the loss function with respect to the parameters of the model. The derivative of the sigmoid function can be represented as:\n",
    "\n",
    "    $$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
    "\n",
    "    Here, `x` is the input to the function. The derivative at a certain point gives the slope of the sigmoid function at that point, which indicates how much the output would change with a small change in the input.\n",
    "\n",
    "3. **Softmax Function**: The softmax function is another type of activation function that is often used in the output layer of a neural network for multi-class classification problems. It maps a vector of real numbers to a vector of probabilities that add up to 1. This means that the output of the softmax function can be interpreted as a probability distribution over the classes. The softmax function is represented as:\n",
    "\n",
    "    $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "    Here, `z_i` is the i-th element of the input vector `Z`, and `K` is the total number of classes. The softmax function ensures that the sum of the output probabilities is 1, which makes it suitable for multi-class classification problems.\n",
    "\n",
    "I hope this provides a clear explanation of these functions! If you have any further questions, feel free to ask. I'm here to help! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef35da",
   "metadata": {},
   "source": [
    "### Step 3(b): Initialize all the parameters in the neural network uniformly\n",
    "\n",
    "In this step, we will initialize all the parameters in the neural network uniformly. The input size is 784 dimensions (each input is a 28x28 image, so we have to flatten the data from 2D to 1D). For the two linear hidden layers, we have 128 and 64 neurons respectively. For the output layer, its size will be 10 since there are 10 classes (0-9) in MNIST.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96adc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value\n",
    "np.random.seed(695)\n",
    "\n",
    "# Function to initialize parameters uniformly\n",
    "def initialize_parameters_uniform(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    W1 = np.random.uniform(low=-0.1, high=0.1, size=(input_size, hidden1_size))\n",
    "    b1 = np.zeros((1, hidden1_size))\n",
    "    W2 = np.random.uniform(low=-0.1, high=0.1, size=(hidden1_size, hidden2_size))\n",
    "    b2 = np.zeros((1, hidden2_size))\n",
    "    W3 = np.random.uniform(low=-0.1, high=0.1, size=(hidden2_size, output_size))\n",
    "    b3 = np.zeros((1, output_size))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    return parameters\n",
    "\n",
    "# Initialize parameters\n",
    "input_layer = 784\n",
    "hidden_layer1 = 128\n",
    "hidden_layer2 = 64\n",
    "output_layer = 10\n",
    "parameters = initialize_parameters_uniform(input_layer, hidden_layer1, hidden_layer2, output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62f2dc",
   "metadata": {},
   "source": [
    "This code first sets the seed value for the random number generator to ensure that we get the same results every time we run the code. It then defines a function to initialize the weights and biases for each layer in the neural network uniformly. The function returns a dictionary containing the weights and biases for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883a1c0",
   "metadata": {},
   "source": [
    "### Discussion for Step 3(b)\n",
    "\n",
    "In this part of the code, I'm setting up the initial parameters for my neural network model. \n",
    "\n",
    "Firstly, I set a seed value for the random number generator using `np.random.seed(695)`. Setting a seed ensures that the random numbers generated are the same every time the code is run. This is important for reproducibility of results.\n",
    "\n",
    "Next, I define a function `initialize_parameters_uniform` to initialize the weights and biases for the neural network. The weights are initialized with small random values from a uniform distribution between -0.1 and 0.1. Initializing the weights with small random values helps to break symmetry and ensures that different neurons learn different features. The biases are initialized to zeros.\n",
    "\n",
    "The function takes as input the sizes of the input layer, two hidden layers, and the output layer, and returns a dictionary containing the initialized parameters. The parameters include:\n",
    "\n",
    "- `W1`, `W2`, `W3`: Weight matrices for the input layer, first hidden layer, and second hidden layer respectively. Each matrix is of size `(previous_layer_size, current_layer_size)`.\n",
    "- `b1`, `b2`, `b3`: Bias vectors for the first hidden layer, second hidden layer, and output layer respectively. Each bias vector is of size `(1, current_layer_size)`.\n",
    "\n",
    "Finally, I call this function to initialize the parameters for my specific neural network architecture. The architecture consists of an input layer of size 784 (since each image is 28x28 pixels), two hidden layers of sizes 128 and 64, and an output layer of size 10 (since there are 10 classes of digits). The initialized parameters are stored in the `parameters` variable, which will be updated during the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64e2b4",
   "metadata": {},
   "source": [
    "### Step 4: Define a function named feed_forward\n",
    "\n",
    "In this step, we will define a function named feed_forward. Given an input x, it should output the sigmoid of wx+b where w and b indicate the weights and bias defined in step 2.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ceba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, parameters):\n",
    "    '''\n",
    "    Propagates the input X in the neural network in the forward\n",
    "    direction using the parameters. It also generates the\n",
    "    forward cache to store the Inputs Z on outputs A of\n",
    "    every layer in the NN architecture.\n",
    "    '''\n",
    "    # Storing the inputs Z of each layer in the cache.\n",
    "    # Storing the activations A of each layer\n",
    "    forward_cache = {}\n",
    "    \n",
    "    # Z = X.W + b\n",
    "    # A = S(z)\n",
    "    \n",
    "    # Flatten the input data\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "    # Hidden layer 1:\n",
    "    Z1 = X.dot(parameters['W1']) + parameters['b1']       # Input to hidden Layer\n",
    "    A1 = sigmoid(Z1)    # Output of the hidden layer\n",
    "    forward_cache[\"Z1\"] = Z1\n",
    "    forward_cache[\"A1\"] = A1\n",
    "\n",
    "    # Hidden layer 2:\n",
    "    Z2 = A1.dot(parameters['W2']) + parameters['b2']       \n",
    "    A2 = sigmoid(Z2)\n",
    "    forward_cache[\"Z2\"] = Z2\n",
    "    forward_cache[\"A2\"] = A2\n",
    "\n",
    "    # Output layer:\n",
    "    Z3 = A2.dot(parameters['W3']) + parameters['b3']       \n",
    "    A3 = softmax(Z3)\n",
    "    forward_cache[\"Z3\"] = Z3\n",
    "    forward_cache[\"A3\"] = A3\n",
    "\n",
    "    return A3, forward_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d29fb",
   "metadata": {},
   "source": [
    "This code defines a function feed_forward that takes as input the data X and the parameters of the neural network. It computes the inputs Z and outputs A for each layer in the neural network and stores them in a cache. The function returns the output of the last layer and the cache.\n",
    "\n",
    "### Discussion for Step 4\n",
    "\n",
    "The `feed_forward` function is responsible for the forward propagation step of the neural network. It takes the input data `X` and the parameters of the network, and computes the output of the network. It also stores the intermediate values in a cache for use in the backward propagation step.\n",
    "\n",
    "1. **Flatten the input data**: The input data `X` is reshaped to ensure it's in the correct format for the matrix operations that follow. This is necessary because the images in the dataset are 2D, but we need to flatten them into 1D vectors to perform the matrix operations.\n",
    "\n",
    "2. **Hidden layer 1**: The input data is passed through the first hidden layer of the network. The input to the layer (`Z1`) is calculated by performing a dot product between the input data and the weights of the first layer, and then adding the bias. The output of the layer (`A1`) is calculated by applying the sigmoid activation function to `Z1`. Both `Z1` and `A1` are stored in the cache.\n",
    "\n",
    "3. **Hidden layer 2**: The output from the first hidden layer is passed through the second hidden layer. The process is similar to the first hidden layer, but this time the input is `A1`, and the weights and bias are those of the second layer. The output `A2` is again calculated using the sigmoid activation function, and `Z2` and `A2` are stored in the cache.\n",
    "\n",
    "4. **Output layer**: The output from the second hidden layer is passed through the output layer of the network. The process is similar to the hidden layers, but this time the input is `A2`, and the weights and bias are those of the output layer. The output `A3` is calculated using the softmax activation function, which is commonly used for multi-class classification problems as it gives a probability distribution over the classes. `Z3` and `A3` are stored in the cache.\n",
    "\n",
    "The function then returns `A3`, which is the output of the network and represents the probabilities of each class for each input, and `forward_cache`, which contains the intermediate values needed for the backward propagation step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9296ca",
   "metadata": {},
   "source": [
    "### Step 5(a): Compute the loss for the output layer\n",
    "\n",
    "In this step, we will compute the loss for the output layer using the categorical cross entropy loss function. This is a common loss function for multi-class classification problems.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463aafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    m_samples = y_true.shape[0]\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "    return -np.sum(y_true * np.log(y_pred_clipped))/m_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2625c4",
   "metadata": {},
   "source": [
    "### Discussion for Step 5(a):-\n",
    "\n",
    "This function calculates the categorical cross-entropy loss, which is commonly used in multi-class classification problems.\n",
    "\n",
    "1. **Number of samples**: The variable `m_samples` is assigned the number of samples in the true labels `y_true`. This is done by getting the shape (i.e., the dimensions) of `y_true` and taking the first element, which represents the number of samples.\n",
    "\n",
    "2. **Clipping the predicted values**: The predicted values `y_pred` are clipped to a range between `1e-12` and `1 - 1e-12` using the `np.clip` function. This is done to avoid taking the logarithm of `0` or `1` in the next step, which would result in `-inf` or `nan` values.\n",
    "\n",
    "3. **Calculating the loss**: The categorical cross-entropy loss is calculated by first multiplying the true labels `y_true` with the logarithm of the clipped predicted values `y_pred_clipped`. This is done element-wise, meaning that each element in `y_true` is multiplied with the corresponding element in `np.log(y_pred_clipped)`. The result is a matrix of the same shape as `y_true` and `y_pred_clipped`, where each element represents the loss for one sample and one class.\n",
    "\n",
    "4. **Averaging the loss**: The loss values calculated in the previous step are summed up using `np.sum`, resulting in a single scalar value. This value is then divided by the number of samples `m_samples` to get the average loss across all samples.\n",
    "\n",
    "The function then returns this average loss, which is a measure of how well the model's predictions match the true labels. The lower the loss, the better the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e8fe6",
   "metadata": {},
   "source": [
    "### Step 5(b): Calculate the gradients for the weights and bias for each layer\n",
    "    \n",
    "In this step, we will calculate the gradients for the weights and bias for each layer. We will use the chain rule to compute gradients for previous layers.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee04ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x, y, parameters, cache):\n",
    "    m = x.shape[0]\n",
    "    grads = {}\n",
    "    loss = categorical_cross_entropy(y, cache[\"A3\"])\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    \n",
    "    last_output =  cache[\"A3\"]\n",
    "    hidden2_output = cache[\"A2\"]\n",
    "    hidden1_output = cache[\"A1\"] \n",
    "\n",
    "    output_error = (last_output - y)   \n",
    "    hidden2_error = np.dot(output_error, W3.T) * sigmoid_derivative(cache[\"Z2\"])\n",
    "    hidden1_error = np.dot(hidden2_error, W2.T) * sigmoid_derivative(cache[\"Z1\"])\n",
    "    \n",
    "    # Flatten the input data\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    \n",
    "    grads = {\n",
    "        'dW3': np.dot(hidden2_output.T, output_error)/m,\n",
    "        'db3': np.sum(output_error, axis=0, keepdims=True)/m,\n",
    "        'dW2': np.dot(hidden1_output.T, hidden2_error)/m,\n",
    "        'db2': np.sum(hidden2_error, axis=0, keepdims=True)/m,\n",
    "        'dW1': np.dot(x.T, hidden1_error) / m,\n",
    "        'db1': np.sum(hidden1_error, axis=0, keepdims=True)/m\n",
    "    }\n",
    "\n",
    "    return grads, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f61a1",
   "metadata": {},
   "source": [
    "### Discussion for Step 5(b):-\n",
    "\n",
    "This function is responsible for the backward propagation step of the neural network, which calculates the gradients of the loss with respect to the parameters of the network. Here's a detailed explanation:\n",
    "\n",
    "1. **Number of samples**: The variable `m` is assigned the number of samples in the input data `x`. This is done by getting the shape (i.e., the dimensions) of `x` and taking the first element, which represents the number of samples.\n",
    "\n",
    "2. **Calculate the loss**: The loss is calculated using the `categorical_cross_entropy` function, which takes the true labels `y` and the output of the network `cache[\"A3\"]` as inputs.\n",
    "\n",
    "3. **Retrieve parameters and cache**: The parameters `W1`, `W2`, and `W3` are retrieved from the `parameters` dictionary, and the outputs `A1`, `A2`, and `A3` of the hidden layers and the output layer are retrieved from the `cache`.\n",
    "\n",
    "4. **Calculate errors**: The error of the output layer `output_error` is calculated as the difference between the output of the network and the true labels. The errors of the hidden layers `hidden2_error` and `hidden1_error` are calculated by backpropagating the error from the subsequent layer, multiplying it with the transpose of the weights of the subsequent layer, and applying the derivative of the sigmoid activation function. This is done because the sigmoid function was used as the activation function in the hidden layers.\n",
    "\n",
    "5. **Flatten the input data**: The input data `x` is reshaped to ensure it's in the correct format for the matrix operations that follow. This is necessary because the images in the dataset are 2D, but we need to flatten them into 1D vectors to perform the matrix operations.\n",
    "\n",
    "6. **Calculate gradients**: The gradients of the weights and biases with respect to the loss are calculated. This is done by taking the dot product of the transpose of the output of the previous layer and the error of the current layer, and dividing by the number of samples. The gradients are stored in the `grads` dictionary.\n",
    "\n",
    "The function then returns `grads`, which contains the gradients of the parameters, and `loss`, which is the value of the loss function. These can be used to update the parameters of the network in the gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8aadd",
   "metadata": {},
   "source": [
    "### Step 6(a): Use mini-batch gradient descent to update the parameters\n",
    "\n",
    "In this step, we will use mini-batch gradient descent to update the parameters including weights and bias. A complete training round consists of a feed forward process, back propagation, and parameter update.\n",
    "\n",
    "Here’s the Python code to accomplish this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e19eed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.01):\n",
    "    '''\n",
    "    Simple function that takes input the parameters, and gradients along \n",
    "    with a learning rate to update the parameters.\n",
    "    '''    \n",
    "    parameters['W1'] -= learning_rate * grads['dW1']\n",
    "    parameters['b1'] -= learning_rate * grads['db1']\n",
    "    parameters['W2'] -= learning_rate * grads['dW2']\n",
    "    parameters['b2'] -= learning_rate * grads['db2']\n",
    "    parameters['W3'] -= learning_rate * grads['dW3']\n",
    "    parameters['b3'] -= learning_rate * grads['db3']\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5591a6",
   "metadata": {},
   "source": [
    "### Discussion for Step 6(a):-\n",
    "\n",
    "This function is responsible for updating the parameters of the neural network using the gradients calculated in the backward propagation step and a learning rate.\n",
    "\n",
    "1. **Input parameters**: The function takes as input the current parameters of the network (`parameters`), the gradients of the parameters (`grads`), and a learning rate (`learning_rate`).\n",
    "\n",
    "2. **Update weights and biases**: The function updates each weight and bias in the `parameters` dictionary by subtracting the product of the learning rate and the corresponding gradient. This is done for the weights and biases of all layers (`W1`, `b1`, `W2`, `b2`, `W3`, `b3`). The learning rate determines the size of the steps we take to reach a (local) minimum. In other words, it controls how much we are adjusting the weights of our network with respect to the loss gradient.\n",
    "\n",
    "3. **Return updated parameters**: The function then returns the updated parameters, which will be used in the next forward propagation step.\n",
    "\n",
    "This process is repeated for each mini-batch of the dataset, which is why this method is known as mini-batch gradient descent. The advantage of using mini-batches is that it can lead to faster convergence and can prevent the algorithm from getting stuck in local minima. It's a balance between the efficiency of batch gradient descent and the robustness of stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364c0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Step: Define the accuracy function\n",
    "def __accuracy(parameters, X, y):\n",
    "    '''\n",
    "    Function to calculate the accuracy of the model.\n",
    "    It takes the parameters, input data X and the true labels y.\n",
    "    It calculates the predicted labels and compares them with the true labels to calculate the accuracy.\n",
    "    '''\n",
    "    # Forward propagation\n",
    "    output, _ = feed_forward(X, parameters)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred = np.argmax(output, axis=1)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    \n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff8ebb",
   "metadata": {},
   "source": [
    "### Discussion for 6(a):-\n",
    "\n",
    "This function is responsible for calculating the accuracy of the model's predictions. \n",
    "\n",
    "1. **Input parameters**: The function takes as input the current parameters of the network (`parameters`), the input data (`X`), and the true labels (`y`).\n",
    "\n",
    "2. **Forward propagation**: The function performs a forward propagation step using the `feed_forward` function, which takes the input data and the parameters as inputs. The output of the network is stored in the `output` variable.\n",
    "\n",
    "3. **Convert probabilities to class labels**: The function then converts the output probabilities to class labels. This is done by taking the index of the maximum value along axis 1 (i.e., the row axis) of the output and the true labels. This is because the output of the network is a probability distribution over the classes, and the class with the highest probability is taken as the predicted class. The true labels are also converted to class labels in the same way.\n",
    "\n",
    "4. **Calculate accuracy**: The function then calculates the accuracy of the model's predictions. This is done by comparing the predicted labels (`y_pred`) with the true labels (`y_true`) and calculating the mean of the comparisons. This gives the proportion of correct predictions, which is the accuracy of the model.\n",
    "\n",
    "5. **Return accuracy and predicted labels**: The function then returns the accuracy and the predicted labels. The accuracy can be used to monitor the performance of the model during training, and the predicted labels can be used for further analysis or evaluation of the model.\n",
    "\n",
    "The goal of a classification model is to make accurate predictions on new, unseen data. Therefore, the accuracy of the model on the validation or test set is a good indicator of how well the model will perform in practice. It's important to monitor the accuracy during training to ensure that the model is learning effectively. If the accuracy on the training set is much higher than on the validation set, it could be a sign that the model is overfitting to the training data and might not perform well on new data. Conversely, if the accuracy is low on both sets, it could be a sign that the model is underfitting and not complex enough to capture the patterns in the data. In both cases, adjustments might need to be made to the model or the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887a41a",
   "metadata": {},
   "source": [
    "### Step 6(b): Implement the mini-batch gradient descent\n",
    "\n",
    "In this step, we will implement the mini-batch gradient descent. Mini-batch gradient descent is a variant of gradient descent that computes the gradient and updates the parameters using a mini-batch of samples instead of the entire dataset. This can make the training process faster and less memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b15500cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train, y_train, test_x, test_y, parameters, learning_rate=0.01, epochs=100, batch_size=128):\n",
    "    '''\n",
    "    A function to train the model, learn the network weights/parameters.\n",
    "    This function uses the mini-batch Gradient Descent (GD) Algorithm.\n",
    "    It takes the training samples shuffles them and splits them into mini-batches\n",
    "    each of size batch_size and then trains the network on each batch.\n",
    "    Once all the mini-batches that make up the entire training set is run, it progresses an epoch or iteration.\n",
    "    '''\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    batches = []\n",
    "    Costs_per_epoch = np.zeros(epochs)\n",
    "    \n",
    "    # Create mini-batch indices\n",
    "    for batch_start in range(0, m, batch_size):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batches.append((batch_start, batch_end))\n",
    "    \n",
    "    print(f\"Total No. Of Batches:{len(batches)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        starttime = time.time()\n",
    "        \n",
    "        # Shuffling the batch indices every Epoch\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        X_shuffled = X_train[permutation,:]\n",
    "        Y_shuffled = y_train[permutation,:]\n",
    "\n",
    "        epoch_loss = 0;\n",
    "        for (start, end) in batches:\n",
    "            X_batch = X_shuffled[start:end,:]\n",
    "            y_batch = Y_shuffled[start:end,:]\n",
    "            \n",
    "            # Forward propagation\n",
    "            output, cache = feed_forward(X_batch, parameters)\n",
    "            \n",
    "            # Backward propagation\n",
    "            grads, loss = back_propagation(X_batch, y_batch, parameters, cache)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Calculate epoch report for monitoring\n",
    "        elapsed = time.time() - starttime\n",
    "        test_pred, _ = feed_forward(test_x, parameters)\n",
    "        Costs_per_epoch[epoch] = epoch_loss / (math.ceil(m/batch_size)) #mean cost of all the batches resembles closer to the actual cost of the whole sample set\n",
    "        accuracy, _ = __accuracy(parameters, test_x, test_y)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, time: {elapsed:.2f}s, Loss: {Costs_per_epoch[epoch]:.4f}, Acc: {accuracy*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\nRan All Epochs\")\n",
    "    return parameters, Costs_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c551e5",
   "metadata": {},
   "source": [
    "### Discussion for 6(b):-\n",
    "\n",
    "Here, this function is responsible for training the neural network using the mini-batch gradient descent algorithm.\n",
    "\n",
    "1. **Input parameters**: The function takes as input the training data (`X_train`, `y_train`), the test data (`test_x`, `test_y`), the current parameters of the network (`parameters`), a learning rate (`learning_rate`), the number of epochs (`epochs`), and a batch size (`batch_size`).\n",
    "\n",
    "2. **Initialize variables**: The function initializes a list `batches` to store the indices of the mini-batches, and an array `Costs_per_epoch` to store the loss for each epoch.\n",
    "\n",
    "3. **Create mini-batch indices**: The function creates the indices for the mini-batches by looping over the range from `0` to `m` (the number of training samples) in steps of `batch_size`. For each step, it appends a tuple of the start and end indices to the `batches` list.\n",
    "\n",
    "4. **Loop over epochs**: The function then enters a loop over the number of epochs. For each epoch, it does the following:\n",
    "\n",
    "    - **Shuffle the data**: The function shuffles the indices of the training data and uses these shuffled indices to create shuffled versions of the input data and the true labels.\n",
    "\n",
    "    - **Loop over mini-batches**: The function then enters a loop over the mini-batches. For each mini-batch, it does the following:\n",
    "\n",
    "        - **Extract mini-batch**: The function extracts the mini-batch from the shuffled data and labels.\n",
    "\n",
    "        - **Forward propagation**: The function performs a forward propagation step using the `feed_forward` function, which takes the mini-batch and the parameters as inputs. The output of the network is stored in the `output` variable.\n",
    "\n",
    "        - **Backward propagation**: The function performs a backward propagation step using the `back_propagation` function, which takes the mini-batch, the true labels, the parameters, and the cache from the forward propagation step as inputs. The gradients and the loss are stored in the `grads` and `loss` variables, respectively.\n",
    "\n",
    "        - **Update parameters**: The function updates the parameters using the `update_parameters` function, which takes the parameters, the gradients, and the learning rate as inputs.\n",
    "\n",
    "    - **Calculate epoch report**: After all mini-batches have been processed, the function calculates the elapsed time for the epoch, the average loss for the epoch, and the accuracy of the model on the test data. It then prints this information for monitoring purposes.\n",
    "\n",
    "5. **Return updated parameters and losses**: After all epochs have been completed, the function returns the updated parameters and the losses for each epoch.\n",
    "\n",
    "All in all, think of this function as the mastermind behind the mini-batch gradient descent algorithm. It’s like a clever twist on the classic gradient descent algorithm. Instead of using just one sample (like stochastic gradient descent) or going all out with every single sample (like batch gradient descent), it strikes a balance. It picks a mini-batch of samples at each step for updating the parameters.\n",
    "\n",
    "This approach has its perks! It can speed up the convergence and also helps avoid getting trapped in those pesky local minima. It’s like having the best of both worlds - the efficiency of batch gradient descent and the robustness of stochastic gradient descent.\n",
    "\n",
    "And here’s the fun part - you can play around with the size of these mini-batches. It’s a hyperparameter that we can tune to get the algorithm performing just the way we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66d63e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. Of Batches:469\n",
      "Epoch 1/100, time: 8.33s, Loss: 1.7517, Acc: 74.42%\n",
      "Epoch 2/100, time: 3.94s, Loss: 0.6159, Acc: 87.31%\n",
      "Epoch 3/100, time: 5.42s, Loss: 0.4042, Acc: 89.78%\n",
      "Epoch 4/100, time: 4.19s, Loss: 0.3381, Acc: 90.89%\n",
      "Epoch 5/100, time: 3.94s, Loss: 0.3010, Acc: 92.04%\n",
      "Epoch 6/100, time: 3.70s, Loss: 0.2722, Acc: 92.63%\n",
      "Epoch 7/100, time: 4.59s, Loss: 0.2476, Acc: 92.78%\n",
      "Epoch 8/100, time: 5.45s, Loss: 0.2267, Acc: 93.74%\n",
      "Epoch 9/100, time: 4.53s, Loss: 0.2079, Acc: 94.18%\n",
      "Epoch 10/100, time: 4.28s, Loss: 0.1923, Acc: 94.41%\n",
      "Epoch 11/100, time: 5.31s, Loss: 0.1783, Acc: 95.02%\n",
      "Epoch 12/100, time: 4.10s, Loss: 0.1661, Acc: 95.15%\n",
      "Epoch 13/100, time: 5.31s, Loss: 0.1548, Acc: 95.28%\n",
      "Epoch 14/100, time: 3.70s, Loss: 0.1455, Acc: 95.62%\n",
      "Epoch 15/100, time: 3.70s, Loss: 0.1366, Acc: 95.94%\n",
      "Epoch 16/100, time: 4.38s, Loss: 0.1288, Acc: 96.04%\n",
      "Epoch 17/100, time: 5.80s, Loss: 0.1219, Acc: 95.94%\n",
      "Epoch 18/100, time: 3.83s, Loss: 0.1154, Acc: 96.14%\n",
      "Epoch 19/100, time: 3.79s, Loss: 0.1093, Acc: 96.37%\n",
      "Epoch 20/100, time: 3.87s, Loss: 0.1038, Acc: 96.53%\n",
      "Epoch 21/100, time: 6.00s, Loss: 0.0994, Acc: 96.58%\n",
      "Epoch 22/100, time: 3.65s, Loss: 0.0945, Acc: 96.86%\n",
      "Epoch 23/100, time: 6.52s, Loss: 0.0905, Acc: 96.93%\n",
      "Epoch 24/100, time: 3.71s, Loss: 0.0863, Acc: 96.92%\n",
      "Epoch 25/100, time: 5.86s, Loss: 0.0825, Acc: 97.02%\n",
      "Epoch 26/100, time: 3.84s, Loss: 0.0791, Acc: 96.97%\n",
      "Epoch 27/100, time: 3.55s, Loss: 0.0757, Acc: 97.13%\n",
      "Epoch 28/100, time: 6.09s, Loss: 0.0726, Acc: 97.09%\n",
      "Epoch 29/100, time: 4.70s, Loss: 0.0701, Acc: 97.35%\n",
      "Epoch 30/100, time: 7.11s, Loss: 0.0670, Acc: 97.39%\n",
      "Epoch 31/100, time: 29.72s, Loss: 0.0644, Acc: 97.32%\n",
      "Epoch 32/100, time: 14.45s, Loss: 0.0619, Acc: 97.36%\n",
      "Epoch 33/100, time: 14.79s, Loss: 0.0596, Acc: 97.39%\n",
      "Epoch 34/100, time: 14.93s, Loss: 0.0574, Acc: 97.47%\n",
      "Epoch 35/100, time: 11.67s, Loss: 0.0551, Acc: 97.53%\n",
      "Epoch 36/100, time: 5.85s, Loss: 0.0530, Acc: 97.65%\n",
      "Epoch 37/100, time: 13.02s, Loss: 0.0512, Acc: 97.70%\n",
      "Epoch 38/100, time: 10.80s, Loss: 0.0494, Acc: 97.69%\n",
      "Epoch 39/100, time: 11.46s, Loss: 0.0476, Acc: 97.68%\n",
      "Epoch 40/100, time: 16.40s, Loss: 0.0460, Acc: 97.82%\n",
      "Epoch 41/100, time: 4.04s, Loss: 0.0443, Acc: 97.69%\n",
      "Epoch 42/100, time: 3.74s, Loss: 0.0426, Acc: 97.86%\n",
      "Epoch 43/100, time: 5.83s, Loss: 0.0413, Acc: 97.78%\n",
      "Epoch 44/100, time: 4.78s, Loss: 0.0399, Acc: 97.71%\n",
      "Epoch 45/100, time: 3.97s, Loss: 0.0384, Acc: 97.76%\n",
      "Epoch 46/100, time: 4.17s, Loss: 0.0370, Acc: 97.82%\n",
      "Epoch 47/100, time: 5.22s, Loss: 0.0358, Acc: 97.70%\n",
      "Epoch 48/100, time: 3.67s, Loss: 0.0346, Acc: 97.75%\n",
      "Epoch 49/100, time: 6.18s, Loss: 0.0330, Acc: 97.62%\n",
      "Epoch 50/100, time: 3.72s, Loss: 0.0324, Acc: 97.88%\n",
      "Epoch 51/100, time: 3.98s, Loss: 0.0310, Acc: 97.92%\n",
      "Epoch 52/100, time: 6.96s, Loss: 0.0301, Acc: 97.90%\n",
      "Epoch 53/100, time: 3.73s, Loss: 0.0290, Acc: 97.83%\n",
      "Epoch 54/100, time: 3.70s, Loss: 0.0279, Acc: 97.93%\n",
      "Epoch 55/100, time: 6.17s, Loss: 0.0271, Acc: 97.82%\n",
      "Epoch 56/100, time: 3.79s, Loss: 0.0263, Acc: 97.88%\n",
      "Epoch 57/100, time: 3.55s, Loss: 0.0251, Acc: 97.89%\n",
      "Epoch 58/100, time: 5.83s, Loss: 0.0244, Acc: 97.88%\n",
      "Epoch 59/100, time: 4.74s, Loss: 0.0235, Acc: 97.77%\n",
      "Epoch 60/100, time: 3.88s, Loss: 0.0228, Acc: 97.95%\n",
      "Epoch 61/100, time: 5.60s, Loss: 0.0219, Acc: 97.96%\n",
      "Epoch 62/100, time: 3.69s, Loss: 0.0212, Acc: 97.95%\n",
      "Epoch 63/100, time: 3.98s, Loss: 0.0204, Acc: 97.85%\n",
      "Epoch 64/100, time: 3.90s, Loss: 0.0198, Acc: 97.94%\n",
      "Epoch 65/100, time: 3.66s, Loss: 0.0191, Acc: 97.97%\n",
      "Epoch 66/100, time: 4.87s, Loss: 0.0186, Acc: 97.88%\n",
      "Epoch 67/100, time: 5.47s, Loss: 0.0178, Acc: 98.00%\n",
      "Epoch 68/100, time: 3.56s, Loss: 0.0172, Acc: 97.93%\n",
      "Epoch 69/100, time: 4.13s, Loss: 0.0166, Acc: 97.90%\n",
      "Epoch 70/100, time: 6.48s, Loss: 0.0161, Acc: 97.90%\n",
      "Epoch 71/100, time: 3.62s, Loss: 0.0156, Acc: 97.94%\n",
      "Epoch 72/100, time: 5.83s, Loss: 0.0152, Acc: 97.96%\n",
      "Epoch 73/100, time: 4.40s, Loss: 0.0146, Acc: 97.96%\n",
      "Epoch 74/100, time: 6.01s, Loss: 0.0140, Acc: 97.95%\n",
      "Epoch 75/100, time: 3.93s, Loss: 0.0137, Acc: 97.96%\n",
      "Epoch 76/100, time: 3.87s, Loss: 0.0132, Acc: 98.02%\n",
      "Epoch 77/100, time: 3.57s, Loss: 0.0128, Acc: 97.91%\n",
      "Epoch 78/100, time: 3.68s, Loss: 0.0125, Acc: 97.86%\n",
      "Epoch 79/100, time: 5.81s, Loss: 0.0120, Acc: 97.93%\n",
      "Epoch 80/100, time: 3.73s, Loss: 0.0116, Acc: 97.96%\n",
      "Epoch 81/100, time: 3.91s, Loss: 0.0113, Acc: 97.91%\n",
      "Epoch 82/100, time: 4.35s, Loss: 0.0109, Acc: 97.93%\n",
      "Epoch 83/100, time: 4.93s, Loss: 0.0106, Acc: 97.93%\n",
      "Epoch 84/100, time: 6.03s, Loss: 0.0104, Acc: 97.92%\n",
      "Epoch 85/100, time: 3.59s, Loss: 0.0100, Acc: 97.93%\n",
      "Epoch 86/100, time: 3.63s, Loss: 0.0097, Acc: 97.94%\n",
      "Epoch 87/100, time: 3.56s, Loss: 0.0094, Acc: 97.91%\n",
      "Epoch 88/100, time: 5.48s, Loss: 0.0091, Acc: 97.86%\n",
      "Epoch 89/100, time: 4.56s, Loss: 0.0089, Acc: 97.97%\n",
      "Epoch 90/100, time: 4.67s, Loss: 0.0086, Acc: 97.89%\n",
      "Epoch 91/100, time: 4.41s, Loss: 0.0084, Acc: 97.94%\n",
      "Epoch 92/100, time: 3.70s, Loss: 0.0082, Acc: 97.93%\n",
      "Epoch 93/100, time: 3.83s, Loss: 0.0079, Acc: 97.87%\n",
      "Epoch 94/100, time: 3.88s, Loss: 0.0077, Acc: 97.94%\n",
      "Epoch 95/100, time: 5.51s, Loss: 0.0075, Acc: 97.90%\n",
      "Epoch 96/100, time: 4.54s, Loss: 0.0073, Acc: 97.91%\n",
      "Epoch 97/100, time: 5.84s, Loss: 0.0071, Acc: 97.88%\n",
      "Epoch 98/100, time: 3.65s, Loss: 0.0069, Acc: 97.92%\n",
      "Epoch 99/100, time: 4.39s, Loss: 0.0067, Acc: 97.88%\n",
      "Epoch 100/100, time: 6.51s, Loss: 0.0066, Acc: 97.94%\n",
      "\n",
      "Ran All Epochs\n"
     ]
    }
   ],
   "source": [
    "parameters, Costs = mini_batch_gradient_descent(train_x, train_y, test_x, test_y, parameters.copy(), learning_rate=0.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7d684",
   "metadata": {},
   "source": [
    "### Discussion for 6(b):-\n",
    "\n",
    "This line is where the magic happens - it's where the training of the neural network takes place!\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "- `mini_batch_gradient_descent(train_x, train_y, test_x, test_y, parameters.copy(), learning_rate=0.23)`: This is the function call to `mini_batch_gradient_descent`, which is the function we discussed earlier that trains the neural network using the mini-batch gradient descent algorithm. It takes several arguments:\n",
    "    - `train_x` and `train_y`: These are the training data and their corresponding labels.\n",
    "    - `test_x` and `test_y`: These are the test data and their corresponding labels. They are used to evaluate the performance of the model after each epoch.\n",
    "    - `parameters.copy()`: This is a copy of the initial parameters of the network. We pass a copy because we don't want to modify the original parameters directly.\n",
    "    - `learning_rate=0.23`: This is the learning rate used in the gradient descent algorithm. It controls the size of the steps taken in the parameter space towards the minimum of the loss function.\n",
    "\n",
    "- `parameters, Costs = ...`: The function returns two values:\n",
    "    - `parameters`: These are the updated parameters of the network after training. They can be used to make predictions on new data.\n",
    "    - `Costs`: This is an array that contains the loss for each epoch during training. It can be used to monitor the progress of the training process.\n",
    "\n",
    "So, in essence, this line of code is training the neural network on the training data, evaluating its performance on the test data after each epoch, and storing the final parameters and the history of losses. After this line, we're all set with a trained model ready to make predictions! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fd11a",
   "metadata": {},
   "source": [
    "### Step 7(a): Use your trained neural network to predict the labels of the test dataset and compute the accuracy on the test dataset.\n",
    "\n",
    "After training our neural network model using the mini_batch_gradient_descent function, you can use the trained parameters to predict the labels of the test dataset. The feed_forward function can be used to get the model’s output probabilities for the test dataset. Then, we can convert these probabilities to class labels by taking the index of the maximum probability for each sample. The accuracy of the model can be calculated by comparing these predicted labels with the true labels. Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccabee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 97.94%\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test dataset\n",
    "test_pred, _ = feed_forward(test_x, parameters)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "test_pred_labels = np.argmax(test_pred, axis=1)\n",
    "test_true_labels = np.argmax(test_y, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(test_pred_labels == test_true_labels)\n",
    "print(f\"Accuracy on the test dataset: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242538bf",
   "metadata": {},
   "source": [
    "### Discussion for 7(a):-\n",
    "\n",
    "This is where we put our trained neural network to the test!\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "1. **Predict the labels of the test dataset**: The `feed_forward` function is called with the test data (`test_x`) and the trained parameters (`parameters`). The output of the network (`test_pred`) is a probability distribution over the classes for each test sample.\n",
    "\n",
    "2. **Convert probabilities to class labels**: The predicted class labels (`test_pred_labels`) are obtained by taking the index of the maximum value along axis 1 (i.e., the row axis) of `test_pred`. This is because the output of the network is a probability distribution over the classes, and the class with the highest probability is taken as the predicted class. The true class labels (`test_true_labels`) are obtained in the same way from `test_y`.\n",
    "\n",
    "3. **Calculate accuracy**: The accuracy of the model's predictions is calculated by comparing the predicted labels with the true labels and calculating the mean of the comparisons. This gives the proportion of correct predictions, which is the accuracy of the model.\n",
    "\n",
    "4. **Print accuracy**: The accuracy on the test dataset is then printed. This gives us an idea of how well our model is likely to perform on new, unseen data.\n",
    "\n",
    "So, in essence, this part of the code is evaluating the performance of our trained model on the test dataset. It's like the final exam for our model after all its hard work during training! And the accuracy is the final score, telling us how well our model has learned to classify new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f2eb0",
   "metadata": {},
   "source": [
    "### Step 7(b): Plot some of the misclassified images with their predicted and true labels.\n",
    "\n",
    "To gain insights into why certain images are misclassified, you can visualize these images along with their true and predicted labels. First, you need to find the indices of the misclassified images. Then, you can randomly select a few of these images to plot. Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4f037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the indices of the misclassified images\n",
    "misclassified_indices = np.where(test_pred_labels != test_true_labels)[0]\n",
    "\n",
    "# Randomly select a few of these images\n",
    "num_images_to_show = 5\n",
    "selected_indices = np.random.choice(misclassified_indices, num_images_to_show, replace=False)\n",
    "\n",
    "# Plot the selected images with their predicted and true labels\n",
    "plt.figure(figsize=(10, 10))  # Adjusting figure size for bigger images\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(num_images_to_show, 1, i+1)  # Changed to display images on separate lines\n",
    "    plt.imshow(test_x[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {test_true_labels[idx]}, Pred: {test_pred_labels[idx]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddbc69",
   "metadata": {},
   "source": [
    "### Discussion for 7(b):-\n",
    "\n",
    "This is where we take a closer look at where our model went wrong by examining some of the misclassified images.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "1. **Find the indices of the misclassified images**: The `np.where` function is used to find the indices where the predicted labels (`test_pred_labels`) do not match the true labels (`test_true_labels`). These indices are stored in the `misclassified_indices` variable.\n",
    "\n",
    "2. **Randomly select a few of these images**: The `np.random.choice` function is used to randomly select a specified number (`num_images_to_show`) of these misclassified indices without replacement (`replace=False`). These selected indices are stored in the `selected_indices` variable.\n",
    "\n",
    "3. **Plot the selected images with their predicted and true labels**: A loop is then used to plot each of the selected images along with their predicted and true labels. For each selected index, the corresponding image is reshaped to its original 2D shape (28x28 pixels) and displayed in grayscale (`cmap='gray'`). The title of the plot shows the true and predicted labels for the image.\n",
    "\n",
    "4. **Display the plots**: Finally, `plt.show()` is called to display the plots. This gives us a visual insight into the types of images that the model is having trouble with, which can be useful for improving the model.\n",
    "\n",
    "In essence, this segment of the code serves as an analytical tool, scrutinizing instances of misclassification. It provides valuable insights into the areas where the model falls short, thereby guiding us towards potential improvements in its predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea05dd4",
   "metadata": {},
   "source": [
    "## Conclusive Reflections - Final Discussion\n",
    "\n",
    "- We began our journey with **Data Preparation**, ensuring our data was normalized and appropriately encoded, setting the stage for the subsequent steps.\n",
    "- We then moved on to **Model Initialization**, carefully initializing the parameters of our network, setting up the weights and biases for each layer.\n",
    "- Once our network was set up, we dove into the **Training Process**. This began with the forward propagation step, followed by the calculation of the loss using the categorical cross-entropy function.\n",
    "- The **Backward Propagation** step followed, where we calculated the gradients of the loss with respect to the parameters, allowing our network to learn from its errors and improve its predictions.\n",
    "- We then updated the parameters of our network using the **Mini-Batch Gradient Descent Algorithm**. This iterative process adjusted the parameters in the direction that minimized the loss, gradually improving the performance of our network.\n",
    "- Finally, we evaluated the performance of our trained model on the test dataset in the **Model Evaluation** step. We calculated the accuracy of the model's predictions and examined some of the misclassified images to gain insights into the model's performance.\n",
    "\n",
    "While the project was initially submitted before the deadline, my curiosity and drive for accuracy led me to invest additional time to further refine the model. Despite facing some system challenges and delays, I believe my perseverance paid off, resulting in improved accuracy.\n",
    "\n",
    "In conclusion, this project served as a comprehensive exercise in understanding and implementing a neural network for a classification task. Each step offered its own set of challenges and learnings. Despite the hurdles, the end result was a well-performing model and a deeper understanding of neural networks. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
